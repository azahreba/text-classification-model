{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:54.180000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "\n",
    "mingw_path = r'C:\\Program Files\\mingw-w64\\x86_64-7.1.0-posix-seh-rt_v5-rev0\\mingw64\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from email.parser import Parser\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english', ignore_stopwords=True)\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-z]{3,}')\n",
    "\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "\n",
    "from __future__ import print_function\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import xgboost as xgb\n",
    "import math \n",
    "import gensim\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:54.993000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "docs_path = r'C:\\Users\\Artur_Zahreba\\Desktop\\WorkFusion\\P&G\\sample email June 15\\Docs'\n",
    "\n",
    "doc_types = sorted(listdir(docs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:55.533000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "def getDoc(filePath):\n",
    "    with open(filePath, 'rb') as fp:\n",
    "        msg = BytesParser(policy=policy.default).parse(fp)\n",
    "        msg_text = msg.get_body(preferencelist=('html', 'plain')).get_content()         \n",
    "        \n",
    "        html_doc = msg_text\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        \n",
    "        texts = []\n",
    "        \n",
    "        for block in soup.findAll('p'):\n",
    "            texts.append(block.text)\n",
    "            \n",
    "        text = '\\n'.join(texts).encode('ascii', 'replace').decode('utf-8').replace(\"?\",\"\").strip()\n",
    "\n",
    "        fp.close()\n",
    "        \n",
    "        return (text, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:57.383000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n"
     ]
    }
   ],
   "source": [
    "doc_texts = []\n",
    "doc_msgs = []\n",
    "\n",
    "for doc_type in doc_types:\n",
    "    \n",
    "    filenames = sorted(listdir(docs_path + '\\\\' + doc_type))\n",
    "    filenames = filter(lambda x: re.search(r'\\.msg$', x), filenames)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        filePath = docs_path + '\\\\' + doc_type + '\\\\' + filename\n",
    "        text, msg = getDoc(filePath)\n",
    "        doc_texts.append((doc_type, filename, text))\n",
    "        doc_msgs.append(msg)\n",
    "            \n",
    "print(len(doc_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:58.515000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Approved',\n",
       " 'FW   Approval  BC OND JFM allowance.msg',\n",
       " 'Hi OM \\n\\nPlease help to process. \\n\\n\\n\\nRegards\\nEunice\\n\\n\\nThe Metropolis, 11 North Buona Vista Drive, #21-07, The Metropolis Tower 2, Singapore 138589\\n\\nAll decisions on pricing, promotion, distribution, assortment and shelving are at the sole discretion of the retailer.\\nThis electronic message transmission contains information which may be confidential. The information is intended for the use of the individual or entity named above. If you are not the intended recipient, and have received this electronic transmission in error, please notify sender then delete immediately.\\n\\n\\nFrom: Meng, FanRu Sent: Wednesday, April 26, 2017 7:07 PMTo: Eunice, Chiam <eunice.ec@pg.com>Subject: RE: [Approval] BC OND JFM allowance\\n\\n\\nApproved. \\n\\n\\nBest Regards,\\nFanRu MENG\\n\\nFrom: Eunice, Chiam Sent: Wednesday, April 26, 2017 6:01 PMTo: Meng, FanRu <meng.fa.1@pg.com>Subject: FW: [Approval] BC OND JFM allowance\\n\\nHI Fanru \\n\\nCan you help to approve on behalf\\n\\nRegards\\nEunice\\n\\n\\nThe Metropolis, 11 North Buona Vista Drive, #21-07, The Metropolis Tower 2, Singapore 138589\\n\\nAll decisions on pricing, promotion, distribution, assortment and shelving are at the sole discretion of the retailer.\\nThis electronic message transmission contains information which may be confidential. The information is intended for the use of the individual or entity named above. If you are not the intended recipient, and have received this electronic transmission in error, please notify sender then delete immediately.\\n\\n\\nFrom: Eunice, Chiam Sent: Monday, April 24, 2017 10:57 AMTo: Liang, Anna <liang.an.2@pg.com>Cc: Chia, Ian <chia.ic@pg.com>; Irawaty, Yvone <irawaty.y@pg.com>Subject: [Approval] BC OND JFM allowance\\n\\nHI Anna \\n\\nPlease provide approval for the BC allowance \\n\\nRegards\\nEunice\\n\\n\\nThe Metropolis, 11 North Buona Vista Drive, #21-07, The Metropolis Tower 2, Singapore 138589\\n\\nAll decisions on pricing, promotion, distribution, assortment and shelving are at the sole discretion of the retailer.\\nThis electronic message transmission contains information which may be confidential. The information is intended for the use of the individual or entity named above. If you are not the intended recipient, and have received this electronic transmission in error, please notify sender then delete immediately.\\n\\n\\nFrom: Nino, DaisyAnne Sent: Monday, April 24, 2017 10:24 AMTo: Eunice, Chiam <eunice.ec@pg.com>; Muthuraman, Alagammai <muthuraman.am@pg.com>Cc: Chia, Ian <chia.ic@pg.com>; Irawaty, Yvone <irawaty.y@pg.com>Subject: RE: BC OND JFM allowance\\n\\nHi Eunice,\\n\\nOk to proceed. Thanks ahead.\\n\\nShip-To Code : 2002834052 - PROCTER & GAMBLE INTERNATIONAL SINGAPORE - SGO\\nEAN Code\\nProduct Description\\nUnit Cost (USD)\\nTotalCost (USD)\\nQuantity \\nStocks\\nCTMZ: ASEAN\\n82254038\\nPitera Essence Starter Kit (TH Exclusive)\\n10.20\\n 1,977.93 \\n194\\nOK\\nOK\\n82254056\\nPitera Essence Luxury Kit (TH Exclusive)\\n30.98\\n 6,071.82 \\n196\\nOK\\nOK\\n82254048\\nPitera Essence Deluxe Kit (TH Exclusive)\\n20.00\\n 40.00 \\n2\\nOK\\nOK\\n\\n\\nBest Regards,\\nDaisy Anne G. Nio (Daisy)\\nGlobal SK-II TR\\nProcter & Gamble Intl Operations SA SG Branch\\nT: +65 6712 4022 | nino.d@pg.com\\n\\nUpcoming OOO: 20 to 24 March\\n\\n\\nFrom: Eunice, Chiam Sent: Saturday, April 22, 2017 9:17 PMTo: Muthuraman, Alagammai <muthuraman.am@pg.com>; Nino, DaisyAnne <nino.d@pg.com>Cc: Chia, Ian <chia.ic@pg.com>; Irawaty, Yvone <irawaty.y@pg.com>Subject: BC OND JFM allowance\\n\\nHi Both \\n\\nPlease check stocks\\n\\nRegards\\nEunice\\n\\n\\nThe Metropolis, 11 North Buona Vista Drive, #21-07, The Metropolis Tower 2, Singapore 138589\\n\\nAll decisions on pricing, promotion, distribution, assortment and shelving are at the sole discretion of the retailer.\\nThis electronic message transmission contains information which may be confidential. The information is intended for the use of the individual or entity named above. If you are not the intended recipient, and have received this electronic transmission in error, please notify sender then delete immediately.')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:32:06.500000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['labels_texts.pkl']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lables, _, texts = zip(*doc_texts)\n",
    "joblib.dump((lables, texts), 'labels_texts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.178000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "lables = list(lables)\n",
    "texts = list(texts)\n",
    "# texts = list(map(lambda x: stemmer.stem(x), list(texts)))\n",
    "# texts = list(map(lambda x: tokenizer.tokenize(x), list(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "{0: 'Other', 1: 'Approved'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.183000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "lable_dict = dict(zip(list(set(lables)), range(len(set(lables)))))\n",
    "inv_lable_dict = dict(zip(lable_dict.values(), lable_dict.keys()))\n",
    "print('Number of classes:', len(inv_lable_dict))\n",
    "print(inv_lable_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.188000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "lables_int = list(map(lambda x: lable_dict[x], lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.192000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "train_ind, test_ind = train_test_split(range(len(texts)), test_size = 0.2, random_state = 42)\n",
    "\n",
    "X = np.array(texts)\n",
    "y = np.array(lables_int)\n",
    "\n",
    "X_train = X[train_ind]\n",
    "X_test = X[test_ind]\n",
    "y_train = y[train_ind]\n",
    "y_test = y[test_ind]\n",
    "#, X_test, y_train, y_test = train_test_split(texts, lables_int, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.196000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(164, 2200)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(sublinear_tf=False, max_df=0.8, min_df=5, use_idf=True, \n",
    "                                                             stop_words='english', lowercase=True, max_features=None,\n",
    "                                                             strip_accents = 'unicode', ngram_range=(1,2), norm=u'l2',\n",
    "                                                             smooth_idf=True, token_pattern=r'[a-z\\d]{2,}')\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test =  vectorizer.transform(X_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.201000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_data.pkl']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((X_train, X_test, y_train, y_test), 'xgb_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.208000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<164x2200 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 22820 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, X_ts, y_tr, y_ts = joblib.load('xgb_data.pkl')\n",
    "X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.217000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "D_train = xgb.DMatrix(X_train, label = y_train)\n",
    "D_test = xgb.DMatrix(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.221000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "param ['booster'] = 'gbtree'\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.3\n",
    "param['gamma'] = 0.2\n",
    "param['min_child_weight'] = 1.0\n",
    "#param['scale_pos_weight'] = 1.0*(y_train.shape[0] - y_train.sum())/(y_train.sum()+1)\n",
    "#param['max_delta_step'] = 10\n",
    "param['max_depth'] = 2\n",
    "param['silent'] = 0\n",
    "param['subsample'] = 0.5\n",
    "param['colsample_bytree' ] = 0.5\n",
    "param['colsample_bylevel' ] = 0.5\n",
    "param['alpha' ] = 0.0\n",
    "param['lambda' ] = 5.0\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = len(lable_dict)\n",
    "# param['one_drop'] = 1\n",
    "# param['rate_drop'] =  0.2\n",
    "param['updater']='grow_histmaker,refresh'\n",
    "# param['sample_type'] =  \"weighted\"\n",
    "# param['normalize_type'] = \"forest\"\n",
    "param['eval_metric'] = 'merror'\n",
    "num_round = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.226000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[1]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[2]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[3]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[4]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[5]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[6]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[7]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[8]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[9]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[10]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[11]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[12]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[13]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[14]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[15]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[16]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[17]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[18]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[19]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[20]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[21]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[22]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[23]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[24]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[25]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[26]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[27]\teval-merror:0.190476\teval-merror:0.164634\n",
      "[28]\teval-merror:0.190476\teval-merror:0.158537\n",
      "[29]\teval-merror:0.190476\teval-merror:0.158537\n",
      "[30]\teval-merror:0.190476\teval-merror:0.158537\n",
      "[31]\teval-merror:0.190476\teval-merror:0.152439\n",
      "[32]\teval-merror:0.190476\teval-merror:0.152439\n",
      "[33]\teval-merror:0.190476\teval-merror:0.146341\n",
      "[34]\teval-merror:0.190476\teval-merror:0.140244\n",
      "[35]\teval-merror:0.190476\teval-merror:0.140244\n",
      "[36]\teval-merror:0.190476\teval-merror:0.140244\n",
      "[37]\teval-merror:0.190476\teval-merror:0.134146\n",
      "[38]\teval-merror:0.190476\teval-merror:0.134146\n",
      "[39]\teval-merror:0.190476\teval-merror:0.134146\n",
      "[40]\teval-merror:0.190476\teval-merror:0.134146\n",
      "[41]\teval-merror:0.190476\teval-merror:0.134146\n",
      "[42]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[43]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[44]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[45]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[46]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[47]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[48]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[49]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[50]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[51]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[52]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[53]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[54]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[55]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[56]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[57]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[58]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[59]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[60]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[61]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[62]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[63]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[64]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[65]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[66]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[67]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[68]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[69]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[70]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[71]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[72]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[73]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[74]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[75]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[76]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[77]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[78]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[79]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[80]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[81]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[82]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[83]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[84]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[85]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[86]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[87]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[88]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[89]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[90]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[91]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[92]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[93]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[94]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[95]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[96]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[97]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[98]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[99]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[100]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[101]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[102]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[103]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[104]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[105]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[106]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[107]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[108]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[109]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[110]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[111]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[112]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[113]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[114]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[115]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[116]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[117]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[118]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[119]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[120]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[121]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[122]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[123]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[124]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[125]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[126]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[127]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[128]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[129]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[130]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[131]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[132]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[133]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[134]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[135]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[136]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[137]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[138]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[139]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[140]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[141]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[142]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[143]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[144]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[145]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[146]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[147]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[148]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[149]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[150]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[151]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[152]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[153]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[154]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[155]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[156]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[157]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[158]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[159]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[160]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[161]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[162]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[163]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[164]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[165]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[166]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[167]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[168]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[169]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[170]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[171]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[172]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[173]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[174]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[175]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[176]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[177]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[178]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[179]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[180]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[181]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[182]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[183]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[184]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[185]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[186]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[187]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[188]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[189]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[190]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[191]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[192]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[193]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[194]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[195]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[196]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[197]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[198]\teval-merror:0.190476\teval-merror:0.128049\n",
      "[199]\teval-merror:0.190476\teval-merror:0.128049\n"
     ]
    }
   ],
   "source": [
    "watchlist  = [(D_test,'eval'), (D_train,'eval')]\n",
    "bst = xgb.train(param, D_train, num_round, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095238095238095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.230000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "xxx = bst.predict(D_test)\n",
    "\n",
    "accuracy = 1.0 * list(map(lambda x, y: 1 if x == y else 0, y_test, xxx)).count(1) / len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.238000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "def softmax(array):\n",
    "    e = map(lambda x: sum(list(map(lambda y: math.pow(math.e, y), x))), array)\n",
    "    array = map(lambda x, y: list(map(lambda z: math.pow(math.e, z)/y, x)), array, e)\n",
    "    return np.array(array)\n",
    "\n",
    "def decision (y):\n",
    "    m = max(y)\n",
    "    cc = 0\n",
    "    for i in y:\n",
    "        if i == m:\n",
    "            return cc\n",
    "        cc +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.242000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "bst_probs = softmax(bst.predict(D_test, output_margin=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.247000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "texts = list(map(lambda x: stemmer.stem(x), list(texts)))\n",
    "texts = list(map(lambda x: tokenizer.tokenize(x), texts))\n",
    "# texts = list(map(lambda x: re.sub(r'\\s[ -\\.\\d]+\\s', ' ', ' '.join(x)), texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.250000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# texts = map(lambda x: re.sub(r'(?<=[^\\w])20\\d\\d(?=[^\\w]?)', 'year_mask', x), texts)\n",
    "# texts = map(lambda x: re.sub(r'year_mask +(0[1-9]|1[0-2]) +([0-2][1-9]|3[01])', 'date_mask', x), texts)\n",
    "# texts = map(lambda x: re.sub(r'year_mask +([0-2][1-9]|3[01]) +(0[1-9]|1[0-2])', 'date_mask', x), texts)\n",
    "# texts = map(lambda x: re.sub(r'([0-2][1-9]|3[01]) +(0[1-9]|1[0-2]) +year_mask', 'date_mask', x), texts)\n",
    "# texts = map(lambda x: re.sub(r'(0[1-9]|1[0-2]) +([0-2][1-9]|3[01]) +year_mask', 'date_mask', x), texts)\n",
    "\n",
    "lable_dict = dict(zip(list(set(lables)), range(len(set(lables)))))\n",
    "inv_lable_dict = dict(zip(lable_dict.values(), lable_dict.keys()))\n",
    "\n",
    "lables_int = np.array(list(map(lambda x: lable_dict[x], lables)))\n",
    "\n",
    "# vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(sublinear_tf=False, max_df=1.0, min_df=5, use_idf=True, \n",
    "#                                                              stop_words='english', lowercase=True, max_features=None,\n",
    "#                                                              strip_accents = 'unicode', ngram_range=(1,3), norm=u'l2',\n",
    "#                                                              smooth_idf=True, token_pattern=r'[a-z]{3,}')\n",
    "\n",
    "# X = vectorizer.fit_transform(texts)\n",
    "                      \n",
    "X = np.array(texts)\n",
    "y = lables_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206,)\n",
      "# classes: 2\n",
      "Other       : 35\n",
      "Approved    : 171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.254000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print('# classes:', len(lable_dict))\n",
    "for i in lable_dict.keys():\n",
    "    print(i, ' '*(10 - len(i)), ':', list(lables_int).count(lable_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Other': 0, 'Approved': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.259000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "print(lable_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.262000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "cv = sklearn.model_selection.KFold(n_splits=5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.267000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "param ['booster'] = 'gbtree'\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.3\n",
    "param['gamma'] = 0.1\n",
    "param['min_child_weight'] = 0.1\n",
    "#param['scale_pos_weight'] = 1.0*(y_train.shape[0] - y_train.sum())/(y_train.sum()+1)\n",
    "#param['max_delta_step'] = 10\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['subsample'] = 0.5\n",
    "param['colsample_bytree' ] = 0.5\n",
    "param['colsample_bylevel' ] = 0.5\n",
    "param['alpha' ] = 0.0\n",
    "param['lambda' ] = 5.0\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = len(lable_dict)\n",
    "# param['one_drop'] = 1\n",
    "# param['rate_drop'] =  0.2\n",
    "param['updater']='grow_histmaker,prune'\n",
    "# param['sample_type'] =  \"weighted\"\n",
    "# param['normalize_type'] = \"forest\"\n",
    "param['eval_metric'] = 'merror'\n",
    "num_round = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.276000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 ...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-21ab37b113c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                                                              \u001b[0mstrip_accents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'unicode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                                                              smooth_idf=True, token_pattern=r'[a-z\\d]{2,}')\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \"\"\"\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 839\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 241\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.283000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.288000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.292000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.295000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.299000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.302000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.305000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.310000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.314000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.318000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.321000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.325000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.329000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.334000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.343000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.348000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n",
      "C:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-06-28 16:31:00.360000. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "cc = 0\n",
    "accs = []\n",
    "incorrect_classes = Counter()\n",
    "incorrect_files = Counter()\n",
    "common_mistakes = Counter()\n",
    "for i, j in cv.split(range(len(doc_texts))):\n",
    "    print ('Fold', cc, '...')\n",
    "    X_train, X_test = X[i], X[j]\n",
    "    y_train, y_test = y[i], y[j]\n",
    "    \n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(sublinear_tf=False, max_df=0.8, min_df=5, use_idf=True, \n",
    "                                                             stop_words='english', lowercase=True, max_features=None,\n",
    "                                                             strip_accents = 'unicode', ngram_range=(1,3), norm=u'l2',\n",
    "                                                             smooth_idf=True, token_pattern=r'[a-z\\d]{2,}')\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train = vectorizer.transform(X_train)\n",
    "    X_test =  vectorizer.transform(X_test)\n",
    "    \n",
    "    D_train = xgb.DMatrix(X_train, label = y_train)\n",
    "    D_test = xgb.DMatrix(X_test, label = y_test)\n",
    "    \n",
    "    bst = xgb.train(param, D_train, num_round)\n",
    "    xxx = bst.predict(D_test)\n",
    "\n",
    "    accuracy = 1.0*map(lambda x,y: 1 if x == y else 0, y_test, xxx).count(1)/len(y_test)\n",
    "    accs.append(accuracy)\n",
    "    \n",
    "    for pr in range(len(xxx)):\n",
    "        if xxx[pr] != y_test[pr]:\n",
    "#            print('pred:', inv_lable_dict[xxx[pr]])\n",
    "#            print('true:', inv_lable_dict[y_test[pr]])\n",
    "#            print(doc_texts[j[pr]][1], '\\n')\n",
    "            incorrect_classes[inv_lable_dict[y_test[pr]]] += 1\n",
    "            incorrect_files[doc_texts[j[pr]][1]] += 1\n",
    "            common_mistakes[(inv_lable_dict[xxx[pr]], inv_lable_dict[y_test[pr]])] += 1\n",
    "            #print(doc_texts[j][2])\n",
    "    \n",
    "    print ('---------------------------------\\n', 'Done')\n",
    "    print ('Fold', cc, 'accuracy :\\t', accuracy, '\\n')\n",
    "    cc += 1\n",
    "cv_acc = sum(accs)/len(accs)\n",
    "print ('%s-Fold cross-validation accuracy over %s classes for the basic BST model is:' % (cv.n_splits, len(lable_dict)), cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_stopwords = set(stopwords.words('english') + list(punctuation) + \n",
    "                 [\"pg.com\", \"from\", \"sent\", \"cc\", \"please\", \"thank\", \"subject\"])\n",
    "\n",
    "def getWords(text):\n",
    "    word_sent = [word for word in word_tokenize(text) if word.lower() not in _stopwords]\n",
    "\n",
    "    return word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "approvers = [\"Meng, FanRu\", \"Liang, Anna\"]\n",
    "\n",
    "def getApprovalText(headers):\n",
    "    texts = []\n",
    "    for h in headers:\n",
    "        c = h.nextSibling\n",
    "        \n",
    "        while getattr(c, 'name', None) == 'p':\n",
    "            text = c.text.encode('ascii', 'replace').decode('utf-8').replace(\"?\",\"\").strip()\n",
    "            texts.append(text)\n",
    "            c = c.nextSibling\n",
    "\n",
    "#     return ' '.join(getWords(' '.join(texts).lower()))\n",
    "    return ' '.join([t.strip() for t in texts]).strip()\n",
    "\n",
    "def getDocText(filename):\n",
    "    with open(docs_path + '/' + filename, 'rb') as fp:\n",
    "        msg = BytesParser(policy=policy.default).parse(fp)\n",
    "        msg_text = msg.get_body(preferencelist=('html', 'plain')).get_content()\n",
    "\n",
    "        soup = BeautifulSoup(str(msg_text), \"lxml\")\n",
    "\n",
    "    #     print(len(soup.findAll('div', text = approvers)))\n",
    "        headers = set()\n",
    "        \n",
    "        wordsection = soup.find('div')\n",
    "        \n",
    "#         print(wordsection)\n",
    "        \n",
    "        if wordsection != None:\n",
    "            for candidate in wordsection.findAll('div'):\n",
    "                if candidate.div != None and any(\"From: \"+ appr in candidate.text for appr in approvers):\n",
    "                    headers.add(candidate)\n",
    "        fp.close()\n",
    "        \n",
    "        return getApprovalText(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df = 0.5,\n",
    "    min_df = 2,\n",
    "    stop_words = 'english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(doc_texts)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "number_of_classes = 2\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters = number_of_classes, \n",
    "    init = 'k-means++', \n",
    "    max_iter = 500, \n",
    "    n_init = 1, \n",
    "    verbose = True\n",
    ")\n",
    "km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(km.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = {}\n",
    "for i, cluster in enumerate(km.labels_):\n",
    "    oneDocument = doc_texts[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDocument\n",
    "    else:\n",
    "        text[cluster] += oneDocument\n",
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "counts = {}\n",
    "\n",
    "for cluster in range(number_of_classes):\n",
    "    word_sent = word_tokenize(text[cluster])\n",
    "#     word_sent = [word for word in word_sent if word.lower() not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(20, freq, key=freq.get)\n",
    "    counts[cluster]=freq\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_keys = {}\n",
    "\n",
    "for cluster in range(2):   \n",
    "    other_clusters = list(set(range(2)) - set([cluster]))\n",
    "    keys_other_clusters = set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[0]]))\n",
    "    unique = set(keywords[cluster]) - keys_other_clusters\n",
    "    unique_keys[cluster] = nlargest(10, unique, key=counts[cluster].get)\n",
    "    \n",
    "unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "approval_text = \"I approve on behalf of Anna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 3)\n",
    "classifier.fit(X, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = vectorizer.transform([approval_text.encode('ascii', errors='ignore').decode('utf8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want to print a priview of the message content, we can extract whatever\n",
    "# the least formatted payload is and print the first three lines.  Of course,\n",
    "# if the message has no plain text part printing the first three lines of html\n",
    "# is probably useless, but this is just a conceptual example.\n",
    "simplest = msg.get_body(preferencelist=('related', 'html', 'plain'))\n",
    "print()\n",
    "email_full_text = ''.join(simplest.get_content().splitlines(keepends=True)[:120])\n",
    "print(email__full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
